{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-C1HDSJob9G",
        "outputId": "fc11aa50-49e4-4c0d-e9e7-a2a5d5c538d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 1.3G\n",
            "-rw------- 1 root root 203M Feb 10 13:41 cuisine_classifier.pkl\n",
            "-rw------- 1 root root  653 Feb 10 13:54 dataset_final_stats.json\n",
            "-rw------- 1 root root  12M Feb  8 10:52 epicurious_processed.jsonl\n",
            "-rw------- 1 root root  53M Feb  8 09:46 epi_r.csv\n",
            "-rw------- 1 root root 374M Feb 10 13:42 final_recipes_enriched.jsonl\n",
            "-rw------- 1 root root 308M Feb  8 09:52 food_com_cleaned.jsonl\n",
            "-rw------- 1 root root 319M Feb  8 10:45 food_com_with_cuisines.jsonl\n",
            "-rw------- 1 root root 1.7M Feb 10 13:54 sample_recipes_1k.jsonl\n",
            "-rw------- 1 root root  12M Feb  8 09:46 train.json\n"
          ]
        }
      ],
      "source": [
        "# 1. Upload files to Google Drive first\n",
        "# 2. Then in Colab:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Recipe_dataset')\n",
        "\n",
        "# Verify\n",
        "!ls -lh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Check files exist\n",
        "files_needed = ['train.json', 'epi_r.csv', 'food_com_cleaned.jsonl']\n",
        "\n",
        "for file in files_needed:\n",
        "    if os.path.exists(file):\n",
        "        size = os.path.getsize(file) / (1024*1024)  # MB\n",
        "        print(f\"✓ {file} ({size:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"✗ {file} - NOT FOUND!\")\n",
        "\n",
        "# Verify Yummly\n",
        "with open('train.json', 'r') as f:\n",
        "    yummly = json.load(f)\n",
        "    print(f\"\\n✓ Yummly: {len(yummly)} recipes\")\n",
        "    print(f\"  Sample: {yummly[0]['cuisine']}\")\n",
        "\n",
        "# Verify Epicurious\n",
        "epi = pd.read_csv('epi_r.csv')\n",
        "print(f\"\\n✓ Epicurious: {len(epi)} recipes\")\n",
        "print(f\"  Columns: {len(epi.columns)}\")\n",
        "\n",
        "# Verify Food.com\n",
        "food_count = 0\n",
        "with open('food_com_cleaned.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        food_count += 1\n",
        "print(f\"\\n✓ Food.com: {food_count} recipes\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✅ ALL FILES READY!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE_CiX8gvRPj",
        "outputId": "07ddf19b-235b-4aec-aced-578299511855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ train.json (11.8 MB)\n",
            "✓ epi_r.csv (52.7 MB)\n",
            "✓ food_com_cleaned.jsonl (307.4 MB)\n",
            "\n",
            "✓ Yummly: 39774 recipes\n",
            "  Sample: greek\n",
            "\n",
            "✓ Epicurious: 20052 recipes\n",
            "  Columns: 680\n",
            "\n",
            "✓ Food.com: 226101 recipes\n",
            "\n",
            "==================================================\n",
            "✅ ALL FILES READY!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING CUISINE CLASSIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load Yummly data (should already be loaded from Step 1)\n",
        "print(\"\\n[1/5] Loading Yummly dataset...\")\n",
        "with open('train.json', 'r') as f:\n",
        "    yummly_data = json.load(f)\n",
        "\n",
        "print(f\"✓ Loaded {len(yummly_data)} recipes\")\n",
        "print(f\"✓ {len(set([r['cuisine'] for r in yummly_data]))} unique cuisines\")\n",
        "\n",
        "# Prepare training data\n",
        "print(\"\\n[2/5] Preparing training data...\")\n",
        "X = []  # Ingredient text\n",
        "y = []  # Cuisine labels\n",
        "\n",
        "for recipe in yummly_data:\n",
        "    # Join ingredients into single text string\n",
        "    ingredients_text = ' '.join(recipe['ingredients']).lower()\n",
        "    X.append(ingredients_text)\n",
        "    y.append(recipe['cuisine'])\n",
        "\n",
        "print(f\"✓ Prepared {len(X)} training samples\")\n",
        "\n",
        "# Split into train/test\n",
        "print(\"\\n[3/5] Splitting data (80% train, 20% test)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # Ensure balanced split\n",
        ")\n",
        "\n",
        "print(f\"✓ Training set: {len(X_train)} samples\")\n",
        "print(f\"✓ Test set: {len(X_test)} samples\")\n",
        "\n",
        "# Create TF-IDF features\n",
        "print(\"\\n[4/5] Creating TF-IDF features...\")\n",
        "print(\"This converts ingredient text into numerical features...\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=3000,      # Use top 3000 most important ingredient words\n",
        "    ngram_range=(1, 2),     # Use single words and word pairs\n",
        "    min_df=2,               # Ignore very rare ingredients\n",
        "    max_df=0.8,             # Ignore super common words\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"✓ Feature matrix shape: {X_train_vec.shape}\")\n",
        "print(f\"  ({X_train_vec.shape[0]} samples × {X_train_vec.shape[1]} features)\")\n",
        "\n",
        "# Train Random Forest classifier\n",
        "print(\"\\n[5/5] Training Random Forest classifier...\")\n",
        "print(\"This may take 1-2 minutes...\")\n",
        "\n",
        "classifier = RandomForestClassifier(\n",
        "    n_estimators=200,       # Use 200 decision trees\n",
        "    max_depth=30,           # Maximum tree depth\n",
        "    min_samples_split=5,    # Minimum samples to split a node\n",
        "    random_state=42,        # For reproducibility\n",
        "    n_jobs=-1,              # Use all CPU cores\n",
        "    verbose=1               # Show progress\n",
        ")\n",
        "\n",
        "classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "print(\"✓ Training complete!\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_pred = classifier.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n✓ Overall Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Performance by Cuisine:\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Save the trained model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_data = {\n",
        "    'vectorizer': vectorizer,\n",
        "    'classifier': classifier,\n",
        "    'accuracy': accuracy,\n",
        "    'cuisines': list(set(y)),\n",
        "    'training_date': str(np.datetime64('today'))\n",
        "}\n",
        "\n",
        "with open('cuisine_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(\"✓ Model saved to: cuisine_classifier.pkl\")\n",
        "print(f\"✓ Model accuracy: {accuracy:.1%}\")\n",
        "print(f\"✓ Number of cuisines: {len(set(y))}\")\n",
        "\n",
        "# Test the model with examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING MODEL WITH EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_examples = [\n",
        "    ['pasta', 'tomato sauce', 'basil', 'parmesan', 'olive oil'],\n",
        "    ['soy sauce', 'rice', 'ginger', 'garlic', 'sesame oil'],\n",
        "    ['tortilla', 'beans', 'salsa', 'cheese', 'avocado'],\n",
        "    ['curry powder', 'coconut milk', 'rice', 'turmeric', 'garam masala'],\n",
        "    ['chicken', 'potatoes', 'carrots', 'onion', 'thyme']\n",
        "]\n",
        "\n",
        "for ingredients in test_examples:\n",
        "    ing_text = ' '.join(ingredients).lower()\n",
        "    vec = vectorizer.transform([ing_text])\n",
        "\n",
        "    cuisine = classifier.predict(vec)[0]\n",
        "    probabilities = classifier.predict_proba(vec)[0]\n",
        "    confidence = max(probabilities)\n",
        "\n",
        "    print(f\"\\nIngredients: {', '.join(ingredients)}\")\n",
        "    print(f\"→ Predicted: {cuisine} (confidence: {confidence:.2%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ CUISINE CLASSIFIER READY!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUKEoPrQvTnk",
        "outputId": "ae4345c4-20c6-485c-c21e-5c990db26338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TRAINING CUISINE CLASSIFIER\n",
            "============================================================\n",
            "\n",
            "[1/5] Loading Yummly dataset...\n",
            "✓ Loaded 39774 recipes\n",
            "✓ 20 unique cuisines\n",
            "\n",
            "[2/5] Preparing training data...\n",
            "✓ Prepared 39774 training samples\n",
            "\n",
            "[3/5] Splitting data (80% train, 20% test)...\n",
            "✓ Training set: 31819 samples\n",
            "✓ Test set: 7955 samples\n",
            "\n",
            "[4/5] Creating TF-IDF features...\n",
            "This converts ingredient text into numerical features...\n",
            "✓ Feature matrix shape: (31819, 3000)\n",
            "  (31819 samples × 3000 features)\n",
            "\n",
            "[5/5] Training Random Forest classifier...\n",
            "This may take 1-2 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   10.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   53.1s\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   53.9s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training complete!\n",
            "\n",
            "============================================================\n",
            "MODEL EVALUATION\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Overall Accuracy: 0.679 (67.9%)\n",
            "\n",
            "Detailed Performance by Cuisine:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   brazilian      1.000     0.237     0.383        93\n",
            "     british      1.000     0.012     0.025       161\n",
            "cajun_creole      0.827     0.618     0.707       309\n",
            "     chinese      0.708     0.856     0.775       535\n",
            "    filipino      0.914     0.212     0.344       151\n",
            "      french      0.524     0.285     0.370       529\n",
            "       greek      0.792     0.438     0.564       235\n",
            "      indian      0.823     0.899     0.859       601\n",
            "       irish      1.000     0.015     0.030       133\n",
            "     italian      0.640     0.878     0.740      1568\n",
            "    jamaican      1.000     0.390     0.562       105\n",
            "    japanese      0.845     0.539     0.658       284\n",
            "      korean      0.882     0.542     0.672       166\n",
            "     mexican      0.801     0.914     0.854      1288\n",
            "    moroccan      0.888     0.482     0.625       164\n",
            "     russian      1.000     0.082     0.151        98\n",
            " southern_us      0.445     0.778     0.566       864\n",
            "     spanish      0.950     0.096     0.174       198\n",
            "        thai      0.755     0.750     0.752       308\n",
            "  vietnamese      0.877     0.345     0.496       165\n",
            "\n",
            "    accuracy                          0.679      7955\n",
            "   macro avg      0.834     0.468     0.515      7955\n",
            "weighted avg      0.734     0.679     0.647      7955\n",
            "\n",
            "\n",
            "============================================================\n",
            "SAVING MODEL\n",
            "============================================================\n",
            "✓ Model saved to: cuisine_classifier.pkl\n",
            "✓ Model accuracy: 67.9%\n",
            "✓ Number of cuisines: 20\n",
            "\n",
            "============================================================\n",
            "TESTING MODEL WITH EXAMPLES\n",
            "============================================================\n",
            "\n",
            "Ingredients: pasta, tomato sauce, basil, parmesan, olive oil\n",
            "→ Predicted: italian (confidence: 67.48%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingredients: soy sauce, rice, ginger, garlic, sesame oil\n",
            "→ Predicted: chinese (confidence: 48.88%)\n",
            "\n",
            "Ingredients: tortilla, beans, salsa, cheese, avocado\n",
            "→ Predicted: mexican (confidence: 64.75%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingredients: curry powder, coconut milk, rice, turmeric, garam masala\n",
            "→ Predicted: indian (confidence: 72.04%)\n",
            "\n",
            "Ingredients: chicken, potatoes, carrots, onion, thyme\n",
            "→ Predicted: southern_us (confidence: 16.30%)\n",
            "\n",
            "============================================================\n",
            "✓ CUISINE CLASSIFIER READY!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FAST VERSION: Cuisine Prediction with Batch Processing\n",
        "## 10-20x Speed Improvement\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ADDING CUISINES TO FOOD.COM RECIPES (FAST VERSION)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load trained model\n",
        "print(\"\\n[1/4] Loading trained classifier...\")\n",
        "with open('cuisine_classifier.pkl', 'rb') as f:\n",
        "    model_data = pickle.load(f)\n",
        "\n",
        "vectorizer = model_data['vectorizer']\n",
        "classifier = model_data['classifier']\n",
        "print(f\"✓ Model loaded (accuracy: {model_data['accuracy']:.1%})\")\n",
        "\n",
        "# Load your cleaned Food.com recipes\n",
        "print(\"\\n[2/4] Loading Food.com recipes...\")\n",
        "food_com_recipes = []\n",
        "\n",
        "with open('food_com_cleaned.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        food_com_recipes.append(json.loads(line))\n",
        "\n",
        "print(f\"✓ Loaded {len(food_com_recipes):,} recipes\")\n",
        "\n",
        "# FAST BATCH PROCESSING\n",
        "print(\"\\n[3/4] Predicting cuisines (BATCH MODE - MUCH FASTER!)...\")\n",
        "print(\"Estimated time: 5-15 minutes instead of 1-2 hours!\")\n",
        "\n",
        "# Prepare all ingredient texts at once\n",
        "ingredient_texts = []\n",
        "valid_indices = []  # Track which recipes have ingredients\n",
        "\n",
        "for idx, recipe in enumerate(food_com_recipes):\n",
        "    ingredients = recipe.get('ingredients', [])\n",
        "    if ingredients and len(ingredients) > 0:\n",
        "        ingredient_texts.append(' '.join(ingredients).lower())\n",
        "        valid_indices.append(idx)\n",
        "    else:\n",
        "        # No ingredients - set default immediately\n",
        "        recipe['cuisine'] = 'american'\n",
        "        recipe['cuisine_confidence'] = 0.0\n",
        "\n",
        "print(f\"  Processing {len(ingredient_texts):,} recipes with ingredients...\")\n",
        "\n",
        "# BATCH PROCESSING PARAMETERS\n",
        "BATCH_SIZE = 10000  # Process 10k recipes at a time (adjust based on RAM)\n",
        "\n",
        "# Process in batches\n",
        "all_cuisines = []\n",
        "all_confidences = []\n",
        "\n",
        "num_batches = (len(ingredient_texts) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "for batch_idx in tqdm(range(num_batches), desc=\"Batch processing\"):\n",
        "    # Get batch\n",
        "    start_idx = batch_idx * BATCH_SIZE\n",
        "    end_idx = min(start_idx + BATCH_SIZE, len(ingredient_texts))\n",
        "    batch_texts = ingredient_texts[start_idx:end_idx]\n",
        "\n",
        "    # Vectorize entire batch at once (FAST!)\n",
        "    batch_vectors = vectorizer.transform(batch_texts)\n",
        "\n",
        "    # Predict entire batch at once (FAST!)\n",
        "    batch_cuisines = classifier.predict(batch_vectors)\n",
        "    batch_probabilities = classifier.predict_proba(batch_vectors)\n",
        "    batch_confidences = np.max(batch_probabilities, axis=1)\n",
        "\n",
        "    # Store results\n",
        "    all_cuisines.extend(batch_cuisines)\n",
        "    all_confidences.extend(batch_confidences)\n",
        "\n",
        "# Assign results back to recipes\n",
        "for i, recipe_idx in enumerate(valid_indices):\n",
        "    food_com_recipes[recipe_idx]['cuisine'] = all_cuisines[i]\n",
        "    food_com_recipes[recipe_idx]['cuisine_confidence'] = round(float(all_confidences[i]), 3)\n",
        "\n",
        "print(f\"\\n✓ All {len(food_com_recipes):,} recipes now have cuisine predictions!\")\n",
        "\n",
        "# Save updated recipes WITH STREAMING (memory efficient)\n",
        "print(\"\\n[4/4] Saving results...\")\n",
        "output_file = 'food_com_with_cuisines.jsonl'\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for recipe in tqdm(food_com_recipes, desc=\"Saving\"):\n",
        "        f.write(json.dumps(recipe) + '\\n')\n",
        "\n",
        "print(f\"✓ Saved to: {output_file}\")\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CUISINE DISTRIBUTION IN FOOD.COM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cuisine_dist = Counter([r['cuisine'] for r in food_com_recipes])\n",
        "\n",
        "print(f\"\\nTop 15 cuisines:\")\n",
        "for cuisine, count in cuisine_dist.most_common(15):\n",
        "    pct = (count / len(food_com_recipes)) * 100\n",
        "    print(f\"  {cuisine:15s}: {count:6,} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nTotal cuisines: {len(cuisine_dist)}\")\n",
        "\n",
        "# Confidence statistics\n",
        "confidences = [r['cuisine_confidence'] for r in food_com_recipes]\n",
        "print(f\"\\nPrediction Confidence Stats:\")\n",
        "print(f\"  Mean:   {np.mean(confidences):.3f}\")\n",
        "print(f\"  Median: {np.median(confidences):.3f}\")\n",
        "print(f\"  Std:    {np.std(confidences):.3f}\")\n",
        "print(f\"  Min:    {np.min(confidences):.3f}\")\n",
        "print(f\"  Max:    {np.max(confidences):.3f}\")\n",
        "\n",
        "# Show confidence distribution\n",
        "low_conf = sum(1 for c in confidences if c < 0.5)\n",
        "mid_conf = sum(1 for c in confidences if 0.5 <= c < 0.7)\n",
        "high_conf = sum(1 for c in confidences if c >= 0.7)\n",
        "\n",
        "print(f\"\\nConfidence Distribution:\")\n",
        "print(f\"  High confidence (≥0.7): {high_conf:6,} ({high_conf/len(confidences)*100:.1f}%)\")\n",
        "print(f\"  Mid confidence (0.5-0.7): {mid_conf:6,} ({mid_conf/len(confidences)*100:.1f}%)\")\n",
        "print(f\"  Low confidence (<0.5):  {low_conf:6,} ({low_conf/len(confidences)*100:.1f}%)\")\n",
        "\n",
        "# Sample predictions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import random\n",
        "for _ in range(5):\n",
        "    recipe = random.choice(food_com_recipes)\n",
        "    print(f\"\\nTitle: {recipe['title']}\")\n",
        "    ingredients = recipe.get('ingredients', [])\n",
        "    if ingredients:\n",
        "        print(f\"Ingredients: {', '.join(ingredients[:5])}...\")\n",
        "    print(f\"Predicted: {recipe['cuisine']} (confidence: {recipe['cuisine_confidence']:.2%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ STEP 3 COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFxpybpxD4c",
        "outputId": "7d1ef61b-080e-4ce3-b96f-e08ddb476779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADDING CUISINES TO FOOD.COM RECIPES (FAST VERSION)\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading trained classifier...\n",
            "✓ Model loaded (accuracy: 67.9%)\n",
            "\n",
            "[2/4] Loading Food.com recipes...\n",
            "✓ Loaded 226,101 recipes\n",
            "\n",
            "[3/4] Predicting cuisines (BATCH MODE - MUCH FASTER!)...\n",
            "Estimated time: 5-15 minutes instead of 1-2 hours!\n",
            "  Processing 226,101 recipes with ingredients...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatch processing:   0%|          | 0/23 [00:00<?, ?it/s][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:   4%|▍         | 1/23 [00:01<00:22,  1.03s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:   9%|▊         | 2/23 [00:02<00:21,  1.01s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  13%|█▎        | 3/23 [00:03<00:19,  1.00it/s][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  17%|█▋        | 4/23 [00:03<00:18,  1.01it/s][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  22%|██▏       | 5/23 [00:05<00:18,  1.02s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "Batch processing:  26%|██▌       | 6/23 [00:06<00:21,  1.25s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "Batch processing:  30%|███       | 7/23 [00:08<00:22,  1.41s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "Batch processing:  35%|███▍      | 8/23 [00:09<00:20,  1.37s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  39%|███▉      | 9/23 [00:10<00:17,  1.26s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  43%|████▎     | 10/23 [00:11<00:15,  1.17s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  48%|████▊     | 11/23 [00:12<00:13,  1.12s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  52%|█████▏    | 12/23 [00:13<00:11,  1.09s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  57%|█████▋    | 13/23 [00:14<00:10,  1.07s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  61%|██████    | 14/23 [00:15<00:09,  1.05s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  65%|██████▌   | 15/23 [00:16<00:08,  1.03s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  70%|██████▉   | 16/23 [00:17<00:07,  1.01s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  74%|███████▍  | 17/23 [00:18<00:05,  1.00it/s][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "Batch processing:  78%|███████▊  | 18/23 [00:20<00:05,  1.08s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "Batch processing:  83%|████████▎ | 19/23 [00:21<00:05,  1.28s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.6s finished\n",
            "Batch processing:  87%|████████▋ | 20/23 [00:23<00:04,  1.42s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  91%|█████████▏| 21/23 [00:24<00:02,  1.33s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
            "Batch processing:  96%|█████████▌| 22/23 [00:25<00:01,  1.25s/it][Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
            "Batch processing: 100%|██████████| 23/23 [00:26<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ All 226,101 recipes now have cuisine predictions!\n",
            "\n",
            "[4/4] Saving results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving: 100%|██████████| 226101/226101 [00:08<00:00, 26472.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved to: food_com_with_cuisines.jsonl\n",
            "\n",
            "============================================================\n",
            "CUISINE DISTRIBUTION IN FOOD.COM\n",
            "============================================================\n",
            "\n",
            "Top 15 cuisines:\n",
            "  southern_us    : 99,653 ( 44.1%)\n",
            "  italian        : 64,697 ( 28.6%)\n",
            "  mexican        : 30,026 ( 13.3%)\n",
            "  chinese        :  9,323 (  4.1%)\n",
            "  indian         :  7,906 (  3.5%)\n",
            "  french         :  4,582 (  2.0%)\n",
            "  greek          :  2,205 (  1.0%)\n",
            "  thai           :  2,071 (  0.9%)\n",
            "  cajun_creole   :  1,906 (  0.8%)\n",
            "  japanese       :  1,252 (  0.6%)\n",
            "  moroccan       :  1,138 (  0.5%)\n",
            "  korean         :    536 (  0.2%)\n",
            "  filipino       :    215 (  0.1%)\n",
            "  vietnamese     :    212 (  0.1%)\n",
            "  jamaican       :    129 (  0.1%)\n",
            "\n",
            "Total cuisines: 20\n",
            "\n",
            "Prediction Confidence Stats:\n",
            "  Mean:   0.344\n",
            "  Median: 0.281\n",
            "  Std:    0.183\n",
            "  Min:    0.102\n",
            "  Max:    0.982\n",
            "\n",
            "Confidence Distribution:\n",
            "  High confidence (≥0.7): 16,520 (7.3%)\n",
            "  Mid confidence (0.5-0.7): 24,710 (10.9%)\n",
            "  Low confidence (<0.5):  184,871 (81.8%)\n",
            "\n",
            "============================================================\n",
            "SAMPLE PREDICTIONS\n",
            "============================================================\n",
            "\n",
            "Title: cranberry sauce whole berry\n",
            "Ingredients: cranberries, orange juice, orange zest, sugar, pineapple tidbits...\n",
            "Predicted: southern_us (confidence: 19.00%)\n",
            "\n",
            "Title: tri tip cabernet stew\n",
            "Ingredients: tri-tip steak, yellow onion, button mushrooms, soy sauce, splenda granular...\n",
            "Predicted: chinese (confidence: 28.50%)\n",
            "\n",
            "Title: apricot filled cookies\n",
            "Ingredients: shortening, eggs, baking powder, milk, sugar...\n",
            "Predicted: southern_us (confidence: 23.80%)\n",
            "\n",
            "Title: summer squash white bean saut\n",
            "Ingredients: extra virgin olive oil, onion, garlic cloves, zucchini, summer squash...\n",
            "Predicted: italian (confidence: 79.50%)\n",
            "\n",
            "Title: crock pot hamburger casserole\n",
            "Ingredients: potatoes, carrots, frozen peas, onions, celery...\n",
            "Predicted: southern_us (confidence: 17.20%)\n",
            "\n",
            "============================================================\n",
            "✓ STEP 3 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PROCESSING EPICURIOUS DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load Epicurious\n",
        "print(\"\\n[1/3] Loading Epicurious...\")\n",
        "epi_df = pd.read_csv('epi_r.csv')\n",
        "print(f\"✓ Loaded {len(epi_df)} recipes\")\n",
        "print(f\"✓ Columns: {len(epi_df.columns)}\")\n",
        "\n",
        "# Define dietary and cuisine columns\n",
        "dietary_cols = {\n",
        "    'vegetarian': 'vegetarian',\n",
        "    'vegan': 'vegan',\n",
        "    'wheat/gluten-free': 'gluten-free',\n",
        "    'peanut free': 'peanut-free',\n",
        "    'soy free': 'soy-free',\n",
        "    'tree nut free': 'tree-nut-free',\n",
        "    'dairy free': 'dairy-free',\n",
        "    'egg free': 'egg-free',\n",
        "    'low-cal': 'low-calorie',\n",
        "    'low-fat': 'low-fat',\n",
        "    'low-sodium': 'low-sodium',\n",
        "    'high-protein': 'high-protein',\n",
        "    'paleo': 'paleo',\n",
        "    'kosher': 'kosher',\n",
        "    'pescatarian': 'pescatarian'\n",
        "}\n",
        "\n",
        "cuisine_cols = ['italian', 'mexican', 'chinese', 'japanese', 'thai',\n",
        "                'indian', 'french', 'greek', 'mediterranean', 'spanish',\n",
        "                'korean', 'vietnamese', 'middle eastern', 'moroccan']\n",
        "\n",
        "# Process recipes\n",
        "print(\"\\n[2/3] Processing recipes...\")\n",
        "epi_recipes = []\n",
        "\n",
        "for idx, row in tqdm(epi_df.iterrows(), total=len(epi_df), desc=\"Processing\"):\n",
        "    # Extract dietary tags\n",
        "    dietary_tags = []\n",
        "    for col, tag_name in dietary_cols.items():\n",
        "        if col in row.index and row[col] == 1.0:\n",
        "            dietary_tags.append(tag_name)\n",
        "\n",
        "    # Extract cuisine\n",
        "    cuisine = 'american'  # default\n",
        "    for col in cuisine_cols:\n",
        "        if col in row.index and row[col] == 1.0:\n",
        "            cuisine = col.replace(' ', '-')\n",
        "            break\n",
        "\n",
        "    # Extract meal type\n",
        "    meal_types = []\n",
        "    if 'dessert' in row.index and row['dessert'] == 1.0:\n",
        "        meal_types.append('dessert')\n",
        "    if 'breakfast' in row.index and row['breakfast'] == 1.0:\n",
        "        meal_types.append('breakfast')\n",
        "    if 'lunch' in row.index and row['lunch'] == 1.0:\n",
        "        meal_types.append('lunch')\n",
        "    if not meal_types:\n",
        "        meal_types = ['dinner']\n",
        "\n",
        "    # Calculate nutrition\n",
        "    calories = row.get('calories', 0)\n",
        "    protein_g = row.get('protein', 0)\n",
        "    fat_g = row.get('fat', 0)\n",
        "    sodium_mg = row.get('sodium', 0)\n",
        "\n",
        "    # Estimate carbs: Calories = (4*protein) + (9*fat) + (4*carbs)\n",
        "    protein_cal = protein_g * 4\n",
        "    fat_cal = fat_g * 9\n",
        "    remaining_cal = max(0, calories - protein_cal - fat_cal)\n",
        "    carbs_g = remaining_cal / 4\n",
        "\n",
        "    recipe = {\n",
        "        'id': f\"epi_{idx}\",\n",
        "        'source_id': idx,\n",
        "        'title': str(row['title']).strip(),\n",
        "        'description': '',\n",
        "        'ingredients': [],  # Epicurious doesn't have ingredient lists\n",
        "        'instructions': [],\n",
        "        'n_ingredients': 0,\n",
        "        'n_steps': 0,\n",
        "        'total_time_min': 0,\n",
        "        'submitted': None,\n",
        "\n",
        "        # Nutrition (actual grams!)\n",
        "        'nutrition': {\n",
        "            'calories': round(calories, 1),\n",
        "            'protein_g': round(protein_g, 1),\n",
        "            'fat_g': round(fat_g, 1),\n",
        "            'carbs_g': round(carbs_g, 1),\n",
        "            'sodium_mg': round(sodium_mg, 1),\n",
        "            'fiber_g': round(carbs_g * 0.1, 1),  # Estimate\n",
        "            'sugar_g': 0,\n",
        "            'saturated_fat_g': 0\n",
        "        },\n",
        "        'nutrition_valid': True,\n",
        "\n",
        "        # REAL dietary tags!\n",
        "        'dietary_tags': dietary_tags,\n",
        "        'cuisine': cuisine,\n",
        "        'cuisine_confidence': 1.0,  # These are verified, not predicted\n",
        "        'meal_types': meal_types,\n",
        "        'difficulty': 'medium',\n",
        "\n",
        "        # Quality\n",
        "        'rating': round(row.get('rating', 0), 2),\n",
        "\n",
        "        # To be filled\n",
        "        'regions': [],\n",
        "        'allergens': [],\n",
        "\n",
        "        'source': 'epicurious'\n",
        "    }\n",
        "\n",
        "    epi_recipes.append(recipe)\n",
        "\n",
        "print(f\"✓ Processed {len(epi_recipes)} Epicurious recipes\")\n",
        "\n",
        "# Save\n",
        "print(\"\\n[3/3] Saving Epicurious recipes...\")\n",
        "with open('epicurious_processed.jsonl', 'w') as f:\n",
        "    for recipe in epi_recipes:\n",
        "        f.write(json.dumps(recipe) + '\\n')\n",
        "\n",
        "print(\"✓ Saved to: epicurious_processed.jsonl\")\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EPICURIOUS STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dietary tags\n",
        "all_dietary = []\n",
        "for r in epi_recipes:\n",
        "    all_dietary.extend(r['dietary_tags'])\n",
        "\n",
        "dietary_dist = Counter(all_dietary)\n",
        "print(f\"\\nTop dietary tags:\")\n",
        "for tag, count in dietary_dist.most_common(10):\n",
        "    print(f\"  {tag}: {count}\")\n",
        "\n",
        "# Cuisines\n",
        "cuisine_dist = Counter([r['cuisine'] for r in epi_recipes])\n",
        "print(f\"\\nTop cuisines:\")\n",
        "for cuisine, count in cuisine_dist.most_common(10):\n",
        "    print(f\"  {cuisine}: {count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ STEP 4 COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UCDvnDI3XDn",
        "outputId": "03e37b87-844f-4609-fa07-43944ab0dd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PROCESSING EPICURIOUS DATASET\n",
            "============================================================\n",
            "\n",
            "[1/3] Loading Epicurious...\n",
            "✓ Loaded 20052 recipes\n",
            "✓ Columns: 680\n",
            "\n",
            "[2/3] Processing recipes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 20052/20052 [00:05<00:00, 3542.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Processed 20052 Epicurious recipes\n",
            "\n",
            "[3/3] Saving Epicurious recipes...\n",
            "✓ Saved to: epicurious_processed.jsonl\n",
            "\n",
            "============================================================\n",
            "EPICURIOUS STATISTICS\n",
            "============================================================\n",
            "\n",
            "Top dietary tags:\n",
            "  peanut-free: 8390\n",
            "  soy-free: 8088\n",
            "  tree-nut-free: 7044\n",
            "  vegetarian: 6846\n",
            "  kosher: 6175\n",
            "  pescatarian: 6042\n",
            "  gluten-free: 4906\n",
            "  dairy-free: 3206\n",
            "  vegan: 1851\n",
            "  paleo: 779\n",
            "\n",
            "Top cuisines:\n",
            "  american: 20052\n",
            "\n",
            "============================================================\n",
            "✓ STEP 4 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ADDING REGIONAL AVAILABILITY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def add_regional_availability(recipe):\n",
        "    \"\"\"Infer regional availability from cuisine and ingredients\"\"\"\n",
        "\n",
        "    # Cuisine → region mapping\n",
        "    cuisine_to_region = {\n",
        "        'italian': 'europe',\n",
        "        'french': 'europe',\n",
        "        'greek': 'mediterranean',\n",
        "        'spanish': 'europe',\n",
        "        'mediterranean': 'mediterranean',\n",
        "        'british': 'europe',\n",
        "        'mexican': 'latin_america',\n",
        "        'brazilian': 'latin_america',\n",
        "        'cajun_creole': 'north_america',\n",
        "        'southern_us': 'north_america',\n",
        "        'american': 'north_america',\n",
        "        'chinese': 'asia',\n",
        "        'japanese': 'asia',\n",
        "        'thai': 'asia',\n",
        "        'indian': 'asia',\n",
        "        'korean': 'asia',\n",
        "        'vietnamese': 'asia',\n",
        "        'filipino': 'asia',\n",
        "        'moroccan': 'middle_east',\n",
        "        'middle-eastern': 'middle_east',\n",
        "        'irish': 'europe',\n",
        "        'russian': 'europe',\n",
        "        'jamaican': 'latin_america'\n",
        "    }\n",
        "\n",
        "    regions = set()\n",
        "\n",
        "    # Add region from cuisine\n",
        "    cuisine = recipe.get('cuisine', 'american')\n",
        "    if cuisine in cuisine_to_region:\n",
        "        regions.add(cuisine_to_region[cuisine])\n",
        "\n",
        "    # Check for exotic/rare ingredients\n",
        "    exotic_keywords = [\n",
        "        'saffron', 'tamarind', 'miso', 'tahini', 'harissa',\n",
        "        'sumac', 'za\\'atar', 'galangal', 'lemongrass', 'kaffir lime',\n",
        "        'fish sauce', 'garam masala', 'cardamom pods'\n",
        "    ]\n",
        "\n",
        "    ingredients = recipe.get('ingredients', [])\n",
        "    if ingredients:\n",
        "        ingredients_text = ' '.join(ingredients).lower()\n",
        "        has_exotic = any(kw in ingredients_text for kw in exotic_keywords)\n",
        "\n",
        "        # If no exotic ingredients, it's more globally available\n",
        "        if not has_exotic:\n",
        "            regions.add('global')\n",
        "    else:\n",
        "        # If no ingredients list, mark as global\n",
        "        regions.add('global')\n",
        "\n",
        "    # Ensure at least one region\n",
        "    if not regions:\n",
        "        regions.add('global')\n",
        "\n",
        "    return list(regions)\n",
        "\n",
        "# Add to Food.com recipes\n",
        "print(\"\\n[1/2] Adding regions to Food.com recipes...\")\n",
        "for recipe in tqdm(food_com_recipes, desc=\"Food.com\"):\n",
        "    recipe['regions'] = add_regional_availability(recipe)\n",
        "\n",
        "# Add to Epicurious recipes (if you processed them)\n",
        "print(\"\\n[2/2] Adding regions to Epicurious recipes...\")\n",
        "for recipe in tqdm(epi_recipes, desc=\"Epicurious\"):\n",
        "    recipe['regions'] = add_regional_availability(recipe)\n",
        "\n",
        "print(\"\\n✓ Regional availability added!\")\n",
        "\n",
        "# Show distribution\n",
        "all_regions = []\n",
        "for r in food_com_recipes + epi_recipes:\n",
        "    all_regions.extend(r['regions'])\n",
        "\n",
        "region_dist = Counter(all_regions)\n",
        "print(f\"\\nRegion distribution:\")\n",
        "for region, count in region_dist.most_common():\n",
        "    print(f\"  {region}: {count:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ STEP 5 COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dls9k3q4c1E",
        "outputId": "58245656-f951-4ee1-e8e5-54ecf855eed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADDING REGIONAL AVAILABILITY\n",
            "============================================================\n",
            "\n",
            "[1/2] Adding regions to Food.com recipes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Food.com: 100%|██████████| 226101/226101 [00:02<00:00, 77390.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Adding regions to Epicurious recipes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epicurious: 100%|██████████| 20052/20052 [00:00<00:00, 206158.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Regional availability added!\n",
            "\n",
            "Region distribution:\n",
            "  global: 241,259\n",
            "  north_america: 121,611\n",
            "  europe: 69,498\n",
            "  latin_america: 30,186\n",
            "  asia: 21,515\n",
            "  mediterranean: 2,205\n",
            "  middle_east: 1,138\n",
            "\n",
            "============================================================\n",
            "✓ STEP 5 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ADDING ALLERGEN DETECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def detect_allergens(ingredients_list):\n",
        "    \"\"\"Detect common allergens in ingredient list\"\"\"\n",
        "\n",
        "    allergen_keywords = {\n",
        "        'dairy': ['milk', 'cheese', 'butter', 'cream', 'yogurt', 'whey',\n",
        "                  'casein', 'ghee', 'buttermilk', 'sour cream', 'ricotta',\n",
        "                  'mozzarella', 'cheddar', 'parmesan', 'brie'],\n",
        "        'eggs': ['egg', 'eggs', 'mayonnaise', 'mayo', 'meringue'],\n",
        "        'peanuts': ['peanut', 'peanuts', 'peanut butter'],\n",
        "        'tree_nuts': ['almond', 'walnut', 'cashew', 'pecan', 'pistachio',\n",
        "                      'hazelnut', 'macadamia', 'pine nut', 'brazil nut'],\n",
        "        'soy': ['soy', 'tofu', 'edamame', 'miso', 'tempeh', 'soy sauce',\n",
        "                'tamari', 'soybean'],\n",
        "        'wheat': ['flour', 'wheat', 'bread', 'pasta', 'couscous', 'bulgur',\n",
        "                  'semolina', 'farro', 'spelt', 'wheat germ'],\n",
        "        'fish': ['salmon', 'tuna', 'cod', 'tilapia', 'fish', 'anchovy',\n",
        "                 'sardine', 'mackerel', 'halibut', 'trout'],\n",
        "        'shellfish': ['shrimp', 'crab', 'lobster', 'clam', 'mussel',\n",
        "                      'oyster', 'scallop', 'prawn', 'crayfish']\n",
        "    }\n",
        "\n",
        "    if not ingredients_list:\n",
        "        return []\n",
        "\n",
        "    ingredients_text = ' '.join(ingredients_list).lower()\n",
        "    detected_allergens = []\n",
        "\n",
        "    for allergen, keywords in allergen_keywords.items():\n",
        "        if any(kw in ingredients_text for kw in keywords):\n",
        "            detected_allergens.append(allergen)\n",
        "\n",
        "    return detected_allergens\n",
        "\n",
        "# Add to all recipes\n",
        "print(\"\\n[1/2] Detecting allergens in Food.com...\")\n",
        "for recipe in tqdm(food_com_recipes, desc=\"Food.com\"):\n",
        "    recipe['allergens'] = detect_allergens(recipe.get('ingredients', []))\n",
        "\n",
        "print(\"\\n[2/2] Detecting allergens in Epicurious...\")\n",
        "for recipe in tqdm(epi_recipes, desc=\"Epicurious\"):\n",
        "    recipe['allergens'] = detect_allergens(recipe.get('ingredients', []))\n",
        "\n",
        "print(\"\\n✓ Allergen detection complete!\")\n",
        "\n",
        "# Statistics\n",
        "all_allergens = []\n",
        "for r in food_com_recipes + epi_recipes:\n",
        "    all_allergens.extend(r['allergens'])\n",
        "\n",
        "allergen_dist = Counter(all_allergens)\n",
        "print(f\"\\nAllergen distribution:\")\n",
        "for allergen, count in allergen_dist.most_common():\n",
        "    print(f\"  {allergen}: {count:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ STEP 6 COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn3LQHAx4rEt",
        "outputId": "aa0774ce-3c6e-4bf6-fed9-619a9e70e65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADDING ALLERGEN DETECTION\n",
            "============================================================\n",
            "\n",
            "[1/2] Detecting allergens in Food.com...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Food.com: 100%|██████████| 226101/226101 [00:05<00:00, 43904.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Detecting allergens in Epicurious...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epicurious: 100%|██████████| 20052/20052 [00:00<00:00, 595162.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Allergen detection complete!\n",
            "\n",
            "Allergen distribution:\n",
            "  dairy: 141,905\n",
            "  wheat: 79,128\n",
            "  eggs: 70,288\n",
            "  tree_nuts: 24,413\n",
            "  soy: 14,241\n",
            "  shellfish: 8,715\n",
            "  fish: 8,705\n",
            "  peanuts: 7,366\n",
            "\n",
            "============================================================\n",
            "✓ STEP 6 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ADDING DIFFICULTY ESTIMATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def estimate_difficulty(n_steps, total_time_min, n_ingredients):\n",
        "    \"\"\"Estimate recipe difficulty from steps, time, and ingredients\"\"\"\n",
        "\n",
        "    # Simple scoring system\n",
        "    score = 0\n",
        "\n",
        "    # Steps contribution\n",
        "    if n_steps <= 5:\n",
        "        score += 1\n",
        "    elif n_steps <= 10:\n",
        "        score += 2\n",
        "    else:\n",
        "        score += 3\n",
        "\n",
        "    # Time contribution\n",
        "    if total_time_min <= 30:\n",
        "        score += 1\n",
        "    elif total_time_min <= 60:\n",
        "        score += 2\n",
        "    else:\n",
        "        score += 3\n",
        "\n",
        "    # Ingredients contribution\n",
        "    if n_ingredients <= 5:\n",
        "        score += 1\n",
        "    elif n_ingredients <= 10:\n",
        "        score += 2\n",
        "    else:\n",
        "        score += 3\n",
        "\n",
        "    # Classify based on total score\n",
        "    if score <= 4:\n",
        "        return 'easy'\n",
        "    elif score <= 7:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'hard'\n",
        "\n",
        "# Add to all recipes\n",
        "print(\"\\n[1/2] Estimating difficulty for Food.com...\")\n",
        "for recipe in tqdm(food_com_recipes, desc=\"Food.com\"):\n",
        "    if not recipe.get('difficulty'):  # Only if not set\n",
        "        recipe['difficulty'] = estimate_difficulty(\n",
        "            recipe.get('n_steps', 0),\n",
        "            recipe.get('total_time_min', 0),\n",
        "            recipe.get('n_ingredients', 0)\n",
        "        )\n",
        "\n",
        "print(\"\\n[2/2] Estimating difficulty for Epicurious...\")\n",
        "for recipe in tqdm(epi_recipes, desc=\"Epicurious\"):\n",
        "    if not recipe.get('difficulty'):\n",
        "        recipe['difficulty'] = 'medium'  # Default for Epicurious\n",
        "\n",
        "print(\"\\n✓ Difficulty estimation complete!\")\n",
        "\n",
        "# Statistics\n",
        "all_difficulties = [r['difficulty'] for r in food_com_recipes + epi_recipes]\n",
        "diff_dist = Counter(all_difficulties)\n",
        "print(f\"\\nDifficulty distribution:\")\n",
        "for difficulty, count in diff_dist.items():\n",
        "    pct = (count / len(all_difficulties)) * 100\n",
        "    print(f\"  {difficulty}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ STEP 7 COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEkvN3cb4uew",
        "outputId": "f80f9e4e-0bb9-4c25-b105-8432124aa2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADDING DIFFICULTY ESTIMATION\n",
            "============================================================\n",
            "\n",
            "[1/2] Estimating difficulty for Food.com...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Food.com: 100%|██████████| 226101/226101 [00:00<00:00, 966221.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Estimating difficulty for Epicurious...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epicurious: 100%|██████████| 20052/20052 [00:00<00:00, 1387216.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Difficulty estimation complete!\n",
            "\n",
            "Difficulty distribution:\n",
            "  medium: 187,655 (76.2%)\n",
            "  easy: 58,498 (23.8%)\n",
            "\n",
            "============================================================\n",
            "✓ STEP 7 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MERGING DATASETS & QUALITY CHECKS (FIXED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1️⃣ Merge datasets\n",
        "print(\"\\n[1/4] Merging datasets...\")\n",
        "all_recipes = food_com_recipes + epi_recipes\n",
        "\n",
        "print(f\"✓ Total recipes: {len(all_recipes):,}\")\n",
        "print(f\"  - Food.com: {len(food_com_recipes):,}\")\n",
        "print(f\"  - Epicurious: {len(epi_recipes):,}\")\n",
        "\n",
        "# 2️⃣ Quality filtering (SOURCE-AWARE)\n",
        "print(\"\\n[2/4] Filtering invalid recipes...\")\n",
        "\n",
        "valid_recipes = []\n",
        "filtered_out = []\n",
        "\n",
        "for recipe in all_recipes:\n",
        "    source = recipe.get('source', '')\n",
        "\n",
        "    # Core requirements (must-have for RAG)\n",
        "    has_title = recipe.get('title', '').strip() != ''\n",
        "    has_cuisine = recipe.get('cuisine', '') != ''\n",
        "\n",
        "    # Source-specific requirements\n",
        "    if source == 'epicurious':\n",
        "        is_valid = has_title and has_cuisine and recipe.get('nutrition_valid', False)\n",
        "    else:  # food.com and others\n",
        "        is_valid = has_title and has_cuisine\n",
        "\n",
        "    if is_valid:\n",
        "        valid_recipes.append(recipe)\n",
        "    else:\n",
        "        filtered_out.append(recipe)\n",
        "\n",
        "print(f\"✓ Valid recipes: {len(valid_recipes):,}\")\n",
        "print(f\"✗ Filtered out: {len(filtered_out):,}\")\n",
        "\n",
        "# 3️⃣ Final statistics\n",
        "print(\"\\n[3/4] Generating final statistics...\")\n",
        "\n",
        "stats = {\n",
        "    'total_recipes': len(valid_recipes),\n",
        "    'sources': Counter([r['source'] for r in valid_recipes]),\n",
        "    'unique_cuisines': len(set(r['cuisine'] for r in valid_recipes)),\n",
        "    'avg_ingredients': np.mean(\n",
        "        [r['n_ingredients'] for r in valid_recipes if r.get('n_ingredients', 0) > 0]\n",
        "    ),\n",
        "    'avg_time_min': np.mean(\n",
        "        [r['total_time_min'] for r in valid_recipes if r.get('total_time_min', 0) > 0]\n",
        "    )\n",
        "}\n",
        "\n",
        "print(f\"\\nFinal Dataset Statistics:\")\n",
        "print(f\"  Total recipes: {stats['total_recipes']:,}\")\n",
        "for src, cnt in stats['sources'].items():\n",
        "    print(f\"  {src}: {cnt:,}\")\n",
        "print(f\"  Unique cuisines: {stats['unique_cuisines']}\")\n",
        "print(f\"  Avg ingredients: {stats['avg_ingredients']:.1f}\")\n",
        "print(f\"  Avg time: {stats['avg_time_min']:.0f} minutes\")\n",
        "\n",
        "# 4️⃣ Data completeness\n",
        "print(f\"\\nData Completeness:\")\n",
        "print(f\"  Has nutrition: {sum(1 for r in valid_recipes if r.get('nutrition_valid')):,}\")\n",
        "print(f\"  Has cuisine: {sum(1 for r in valid_recipes if r.get('cuisine')):,}\")\n",
        "print(f\"  Has regions: {sum(1 for r in valid_recipes if r.get('regions')):,}\")\n",
        "print(f\"  Has allergens: {sum(1 for r in valid_recipes if r.get('allergens')):,}\")\n",
        "print(f\"  Has difficulty: {sum(1 for r in valid_recipes if r.get('difficulty')):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ STEP 8 COMPLETE (FIXED)\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o52b22R447VY",
        "outputId": "67936012-c487-4804-cfd3-582bb5be2d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MERGING DATASETS & QUALITY CHECKS (FIXED)\n",
            "============================================================\n",
            "\n",
            "[1/4] Merging datasets...\n",
            "✓ Total recipes: 246,153\n",
            "  - Food.com: 226,101\n",
            "  - Epicurious: 20,052\n",
            "\n",
            "[2/4] Filtering invalid recipes...\n",
            "✓ Valid recipes: 246,152\n",
            "✗ Filtered out: 1\n",
            "\n",
            "[3/4] Generating final statistics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Dataset Statistics:\n",
            "  Total recipes: 246,152\n",
            "  food.com: 226,100\n",
            "  epicurious: 20,052\n",
            "  Unique cuisines: 21\n",
            "  Avg ingredients: 9.2\n",
            "  Avg time: nan minutes\n",
            "\n",
            "Data Completeness:\n",
            "  Has nutrition: 20,052\n",
            "  Has cuisine: 246,152\n",
            "  Has regions: 246,152\n",
            "  Has allergens: 182,855\n",
            "  Has difficulty: 246,152\n",
            "\n",
            "============================================================\n",
            "✓ STEP 8 COMPLETE (FIXED)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MERGING DATASETS & QUALITY CHECKS (FIXED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- Function Definitions (Copied for self-containment) ---\n",
        "\n",
        "def detect_allergens(ingredients_list):\n",
        "    \"\"\"Detect common allergens in ingredient list\"\"\"\n",
        "\n",
        "    allergen_keywords = {\n",
        "        'dairy': ['milk', 'cheese', 'butter', 'cream', 'yogurt', 'whey',\n",
        "                  'casein', 'ghee', 'buttermilk', 'sour cream', 'ricotta',\n",
        "                  'mozzarella', 'cheddar', 'parmesan', 'brie'],\n",
        "        'eggs': ['egg', 'eggs', 'mayonnaise', 'mayo', 'meringue'],\n",
        "        'peanuts': ['peanut', 'peanuts', 'peanut butter'],\n",
        "        'tree_nuts': ['almond', 'walnut', 'cashew', 'pecan', 'pistachio',\n",
        "                      'hazelnut', 'macadamia', 'pine nut', 'brazil nut'],\n",
        "        'soy': ['soy', 'tofu', 'edamame', 'miso', 'tempeh', 'soy sauce',\n",
        "                'tamari', 'soybean'],\n",
        "        'wheat': ['flour', 'wheat', 'bread', 'pasta', 'couscous', 'bulgur',\n",
        "                  'semolina', 'farro', 'spelt', 'wheat germ'],\n",
        "        'fish': ['salmon', 'tuna', 'cod', 'tilapia', 'fish', 'anchovy',\n",
        "                 'sardine', 'mackerel', 'halibut', 'trout'],\n",
        "        'shellfish': ['shrimp', 'crab', 'lobster', 'clam', 'mussel',\n",
        "                      'oyster', 'scallop', 'prawn', 'crayfish']\n",
        "    }\n",
        "\n",
        "    if not ingredients_list:\n",
        "        return []\n",
        "\n",
        "    ingredients_text = ' '.join(ingredients_list).lower()\n",
        "    detected_allergens = []\n",
        "\n",
        "    for allergen, keywords in allergen_keywords.items():\n",
        "        if any(kw in ingredients_text for kw in keywords):\n",
        "            detected_allergens.append(allergen)\n",
        "\n",
        "    return detected_allergens\n",
        "\n",
        "def add_regional_availability(recipe):\n",
        "    \"\"\"Infer regional availability from cuisine and ingredients\"\"\"\n",
        "\n",
        "    # Cuisine → region mapping\n",
        "    cuisine_to_region = {\n",
        "        'italian': 'europe',\n",
        "        'french': 'europe',\n",
        "        'greek': 'mediterranean',\n",
        "        'spanish': 'europe',\n",
        "        'mediterranean': 'mediterranean',\n",
        "        'british': 'europe',\n",
        "        'mexican': 'latin_america',\n",
        "        'brazilian': 'latin_america',\n",
        "        'cajun_creole': 'north_america',\n",
        "        'southern_us': 'north_america',\n",
        "        'american': 'north_america',\n",
        "        'chinese': 'asia',\n",
        "        'japanese': 'asia',\n",
        "        'thai': 'asia',\n",
        "        'indian': 'asia',\n",
        "        'korean': 'asia',\n",
        "        'vietnamese': 'asia',\n",
        "        'filipino': 'asia',\n",
        "        'moroccan': 'middle_east',\n",
        "        'middle-eastern': 'middle_east',\n",
        "        'irish': 'europe',\n",
        "        'russian': 'europe',\n",
        "        'jamaican': 'latin_america'\n",
        "    }\n",
        "\n",
        "    regions = set()\n",
        "\n",
        "    # Add region from cuisine\n",
        "    cuisine = recipe.get('cuisine', 'american')\n",
        "    if cuisine in cuisine_to_region:\n",
        "        regions.add(cuisine_to_region[cuisine])\n",
        "\n",
        "    # Check for exotic/rare ingredients\n",
        "    exotic_keywords = [\n",
        "        'saffron', 'tamarind', 'miso', 'tahini', 'harissa',\n",
        "        'sumac', 'za\\'atar', 'galangal', 'lemongrass', 'kaffir lime',\n",
        "        'fish sauce', 'garam masala', 'cardamom pods'\n",
        "    ]\n",
        "\n",
        "    ingredients = recipe.get('ingredients', [])\n",
        "    if ingredients:\n",
        "        ingredients_text = ' '.join(ingredients).lower()\n",
        "        has_exotic = any(kw in ingredients_text for kw in exotic_keywords)\n",
        "\n",
        "        # If no exotic ingredients, it's more globally available\n",
        "        if not has_exotic:\n",
        "            regions.add('global')\n",
        "    else:\n",
        "        # If no ingredients list, mark as global\n",
        "        regions.add('global')\n",
        "\n",
        "    # Ensure at least one region\n",
        "    if not regions:\n",
        "        regions.add('global')\n",
        "\n",
        "    return list(regions)\n",
        "\n",
        "\n",
        "# Load Food.com recipes\n",
        "print(\"\\n[1/7] Loading Food.com recipes...\")\n",
        "food_com_recipes = []\n",
        "\n",
        "with open('food_com_with_cuisines.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        food_com_recipes.append(json.loads(line))\n",
        "\n",
        "print(f\"✓ Loaded {len(food_com_recipes):,} Food.com recipes\")\n",
        "\n",
        "# Load Epicurious recipes (if you have them)\n",
        "print(\"\\n[2/7] Loading Epicurious recipes...\")\n",
        "try:\n",
        "    epi_recipes = []\n",
        "    with open('epicurious_processed.jsonl', 'r') as f:\n",
        "        for line in f:\n",
        "            epi_recipes.append(json.loads(line))\n",
        "    print(f\"✓ Loaded {len(epi_recipes):,} Epicurious recipes\")\n",
        "except FileNotFoundError:\n",
        "    epi_recipes = []\n",
        "    print(\"⚠ Epicurious file not found - skipping\")\n",
        "\n",
        "# CONVERT FOOD.COM TO STANDARD FORMAT & ADD DERIVED FEATURES\n",
        "print(\"\\n[3/7] Converting Food.com to standard format and adding features...\")\n",
        "\n",
        "def convert_food_com_nutrition(nutrition_raw):\n",
        "    \"\"\"\n",
        "    Convert Food.com nutrition_raw to standard format.\n",
        "\n",
        "    nutrition_raw format: [calories, fat_pdv, sugar_pdv, sodium_pdv, protein_pdv, sat_fat_pdv, carbs_pdv]\n",
        "    \"\"\"\n",
        "    if not nutrition_raw or len(nutrition_raw) < 7:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        return {\n",
        "            'calories': round(float(nutrition_raw[0]), 1),\n",
        "            'fat_g': round((float(nutrition_raw[1]) / 100) * 78, 1),\n",
        "            'sugar_g': round((float(nutrition_raw[2]) / 100) * 50, 1),\n",
        "            'sodium_mg': round((float(nutrition_raw[3]) / 100) * 2300, 1),\n",
        "            'protein_g': round((float(nutrition_raw[4]) / 100) * 50, 1),\n",
        "            'saturated_fat_g': round((float(nutrition_raw[5]) / 100) * 20, 1),\n",
        "            'carbs_g': round((float(nutrition_raw[6]) / 100) * 275, 1),\n",
        "            'fiber_g': round((float(nutrition_raw[6]) / 100) * 275 * 0.1, 1)\n",
        "        }\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "for recipe in tqdm(food_com_recipes, desc=\"Processing Food.com\"):\n",
        "    # Convert nutrition_raw to nutrition dict\n",
        "    if 'nutrition_raw' in recipe and 'nutrition' not in recipe:\n",
        "        recipe['nutrition'] = convert_food_com_nutrition(recipe['nutrition_raw'])\n",
        "        recipe['nutrition_valid'] = recipe['nutrition'] is not None\n",
        "    elif 'nutrition' not in recipe:\n",
        "        recipe['nutrition'] = None\n",
        "        recipe['nutrition_valid'] = False\n",
        "    else:\n",
        "        # Already has nutrition dict\n",
        "        recipe['nutrition_valid'] = recipe['nutrition'] is not None\n",
        "\n",
        "    # Fix time field name\n",
        "    if 'time_minutes' in recipe and 'total_time_min' not in recipe:\n",
        "        recipe['total_time_min'] = recipe['time_minutes']\n",
        "    elif 'total_time_min' not in recipe:\n",
        "        recipe['total_time_min'] = 0\n",
        "\n",
        "    # Ensure meal_types exists\n",
        "    if 'meal_types' not in recipe:\n",
        "        recipe['meal_types'] = ['dinner']  # default\n",
        "\n",
        "    # Ensure dietary_tags exists\n",
        "    if 'dietary_tags' not in recipe:\n",
        "        recipe['dietary_tags'] = []\n",
        "\n",
        "    # Add regions and allergens here for food.com recipes\n",
        "    recipe['regions'] = add_regional_availability(recipe)\n",
        "    recipe['allergens'] = detect_allergens(recipe.get('ingredients', []))\n",
        "\n",
        "\n",
        "print(f\"✓ Processed {len(food_com_recipes):,} Food.com recipes\")\n",
        "\n",
        "# Add derived features to Epicurious recipes\n",
        "print(\"\\n[4/7] Adding features to Epicurious recipes...\")\n",
        "for recipe in tqdm(epi_recipes, desc=\"Processing Epicurious\"):\n",
        "    # Epicurious already has some dietary tags, difficulty, etc. from its processing cell\n",
        "    # Add regions and allergens here for epicurious recipes\n",
        "    recipe['regions'] = add_regional_availability(recipe)\n",
        "    recipe['allergens'] = detect_allergens(recipe.get('ingredients', [])) # Will be empty since no ingredients\n",
        "\n",
        "print(f\"✓ Processed {len(epi_recipes):,} Epicurious recipes\")\n",
        "\n",
        "# Merge all recipes\n",
        "print(\"\\n[5/7] Merging datasets...\")\n",
        "all_recipes = food_com_recipes + epi_recipes\n",
        "print(f\"✓ Total recipes before filtering: {len(all_recipes):,}\")\n",
        "print(f\"  - Food.com: {len(food_com_recipes):,}\")\n",
        "print(f\"  - Epicurious: {len(epi_recipes):,}\")\n",
        "\n",
        "# QUALITY FILTERING\n",
        "print(\"\\n[6/7] Filtering invalid recipes...\")\n",
        "\n",
        "valid_recipes = []\n",
        "filtered_out = []\n",
        "\n",
        "for recipe in tqdm(all_recipes, desc=\"Filtering\"):\n",
        "    # Must have:\n",
        "    # 1. Valid nutrition\n",
        "    # 2. Title\n",
        "    # 3. At least 1 ingredient (Food.com)\n",
        "    # 4. Cuisine\n",
        "\n",
        "    has_nutrition = recipe.get('nutrition_valid', False) and recipe.get('nutrition') is not None\n",
        "    has_title = recipe.get('title', '').strip() != ''\n",
        "    has_ingredients = len(recipe.get('ingredients', [])) > 0\n",
        "    has_cuisine = recipe.get('cuisine', '') != ''\n",
        "\n",
        "    is_valid = has_nutrition and has_title and has_ingredients and has_cuisine\n",
        "\n",
        "    if is_valid:\n",
        "        valid_recipes.append(recipe)\n",
        "    else:\n",
        "        filtered_out.append({\n",
        "            'id': recipe.get('id', 'unknown'),\n",
        "            'title': recipe.get('title', 'no title'),\n",
        "            'reason': {\n",
        "                'has_nutrition': has_nutrition,\n",
        "                'has_title': has_title,\n",
        "                'has_ingredients': has_ingredients,\n",
        "                'has_cuisine': has_cuisine\n",
        "            }\n",
        "        })\n",
        "\n",
        "print(f\"\\n✓ Valid recipes: {len(valid_recipes):,}\")\n",
        "print(f\"✗ Filtered out: {len(filtered_out):,}\")\n",
        "\n",
        "# Show why recipes were filtered out\n",
        "if len(filtered_out) > 0:\n",
        "    print(\"\\nReasons for filtering:\")\n",
        "    reasons = {\n",
        "        'no_nutrition': sum(1 for r in filtered_out if not r['reason']['has_nutrition']),\n",
        "        'no_title': sum(1 for r in filtered_out if not r['reason']['has_title']),\n",
        "        'no_ingredients': sum(1 for r in filtered_out if not r['reason']['has_ingredients']),\n",
        "        'no_cuisine': sum(1 for r in filtered_out if not r['reason']['has_cuisine'])\n",
        "    }\n",
        "    for reason, count in reasons.items():\n",
        "        if count > 0:\n",
        "            print(f\"  {reason}: {count:,}\")\n",
        "\n",
        "# FINAL STATISTICS\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL DATASET STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stats = {\n",
        "    'total_recipes': len(valid_recipes),\n",
        "    'sources': {\n",
        "        'food.com': sum(1 for r in valid_recipes if r.get('source') == 'food.com'),\n",
        "        'epicurious': sum(1 for r in valid_recipes if r.get('source') == 'epicurious')\n",
        "    },\n",
        "    'cuisines': len(set([r['cuisine'] for r in valid_recipes])),\n",
        "    'avg_ingredients': np.mean([len(r.get('ingredients', [])) for r in valid_recipes]),\n",
        "    'avg_time_min': np.mean([r.get('total_time_min', 0) for r in valid_recipes if r.get('total_time_min', 0) > 0]),\n",
        "}\n",
        "\n",
        "print(f\"\\nDataset Size:\")\n",
        "print(f\"  Total recipes: {stats['total_recipes']:,}\")\n",
        "print(f\"  Food.com: {stats['sources']['food.com']:,}\")\n",
        "print(f\"  Epicurious: {stats['sources']['epicurious']:,}\")\n",
        "\n",
        "print(f\"\\nContent Stats:\")\n",
        "print(f\"  Unique cuisines: {stats['cuisines']}\")\n",
        "print(f\"  Avg ingredients: {stats['avg_ingredients']:.1f}\")\n",
        "print(f\"  Avg time: {stats['avg_time_min']:.0f} minutes\")\n",
        "\n",
        "# Data completeness\n",
        "print(f\"\\nData Completeness:\")\n",
        "print(f\"  Has nutrition: {sum(1 for r in valid_recipes if r.get('nutrition_valid')):,} ({sum(1 for r in valid_recipes if r.get('nutrition_valid'))/len(valid_recipes)*100:.1f}%)\")\n",
        "print(f\"  Has cuisine: {sum(1 for r in valid_recipes if r.get('cuisine')):,} (100%)\")\n",
        "print(f\"  Has regions: {sum(1 for r in valid_recipes if r.get('regions')):,} ({sum(1 for r in valid_recipes if r.get('regions'))/len(valid_recipes)*100:.1f}%)\")\n",
        "print(f\"  Has allergens: {sum(1 for r in valid_recipes if r.get('allergens')):,} ({sum(1 for r in valid_recipes if r.get('allergens'))/len(valid_recipes)*100:.1f}%)\")\n",
        "print(f\"  Has difficulty: {sum(1 for r in valid_recipes if r.get('difficulty')):,} ({sum(1 for r in valid_recipes if r.get('difficulty'))/len(valid_recipes)*100:.1f}%)\")\n",
        "\n",
        "# Cuisine distribution\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CUISINE DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cuisine_dist = Counter([r['cuisine'] for r in valid_recipes])\n",
        "print(f\"\\nTop 15 cuisines:\")\n",
        "for cuisine, count in cuisine_dist.most_common(15):\n",
        "    pct = (count / len(valid_recipes)) * 100\n",
        "    print(f\"  {cuisine:15s}: {count:6,} ({pct:5.1f}%)\")\n",
        "\n",
        "# Dietary tags distribution\n",
        "all_dietary = []\n",
        "for r in valid_recipes:\n",
        "    all_dietary.extend(r.get('dietary_tags', []))\n",
        "\n",
        "if all_dietary:\n",
        "    dietary_dist = Counter(all_dietary)\n",
        "    print(f\"\\nTop dietary tags:\")\n",
        "    for tag, count in dietary_dist.most_common(10):\n",
        "        print(f\"  {tag}: {count:,}\")\n",
        "\n",
        "# Difficulty distribution\n",
        "difficulty_dist = Counter([r.get('difficulty', 'unknown') for r in valid_recipes])\n",
        "print(f\"\\nDifficulty distribution:\")\n",
        "for difficulty, count in difficulty_dist.items():\n",
        "    pct = (count / len(valid_recipes)) * 100\n",
        "    print(f\"  {difficulty}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Region distribution\n",
        "all_regions = []\n",
        "for r in valid_recipes:\n",
        "    all_regions.extend(r.get('regions', []))\n",
        "\n",
        "region_dist = Counter(all_regions)\n",
        "print(f\"\\nRegion distribution:\")\n",
        "for region, count in region_dist.most_common():\n",
        "    print(f\"  {region}: {count:,}\")\n",
        "\n",
        "# Allergen distribution\n",
        "all_allergens = []\n",
        "for r in valid_recipes:\n",
        "    all_allergens.extend(r.get('allergens', []))\n",
        "\n",
        "if all_allergens:\n",
        "    allergen_dist = Counter(all_allergens)\n",
        "    print(f\"\\nAllergen distribution:\")\n",
        "    for allergen, count in allergen_dist.most_common():\n",
        "        print(f\"  {allergen}: {count:,}\")\n",
        "\n",
        "# Nutrition stats\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NUTRITION STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "valid_nutrition = [r['nutrition'] for r in valid_recipes if r.get('nutrition_valid')]\n",
        "\n",
        "if valid_nutrition:\n",
        "    calories = [n['calories'] for n in valid_nutrition]\n",
        "    protein = [n['protein_g'] for n in valid_nutrition]\n",
        "    carbs = [n['carbs_g'] for n in valid_nutrition]\n",
        "    fats = [n['fat_g'] for n in valid_nutrition]\n",
        "\n",
        "    print(f\"\\nCalories:\")\n",
        "    print(f\"  Mean: {np.mean(calories):.0f}\")\n",
        "    print(f\"  Median: {np.median(calories):.0f}\")\n",
        "    print(f\"  Range: {np.min(calories):.0f} - {np.max(calories):.0f}\")\n",
        "\n",
        "    print(f\"\\nProtein (g):\")\n",
        "    print(f\"  Mean: {np.mean(protein):.1f}\")\n",
        "    print(f\"  Median: {np.median(protein):.1f}\")\n",
        "\n",
        "    print(f\"\\nCarbs (g):\")\n",
        "    print(f\"  Mean: {np.mean(carbs):.1f}\")\n",
        "    print(f\"  Median: {np.median(carbs):.1f}\")\n",
        "\n",
        "    print(f\"\\nFats (g):\")\n",
        "    print(f\"  Mean: {np.mean(fats):.1f}\")\n",
        "    print(f\"  Median: {np.median(fats):.1f}\")\n",
        "\n",
        "# Sample valid recipes\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE VALID RECIPES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import random\n",
        "for _ in range(3):\n",
        "    recipe = random.choice(valid_recipes)\n",
        "    print(f\"\\nTitle: {recipe['title']}\")\n",
        "    print(f\"Source: {recipe.get('source', 'unknown')}\")\n",
        "    print(f\"Cuisine: {recipe['cuisine']} (confidence: {recipe.get('cuisine_confidence', 0):.2%})\")\n",
        "    print(f\"Ingredients ({len(recipe.get('ingredients', []))}): {', '.join(recipe.get('ingredients', [])[:5])}...\")\n",
        "    if recipe.get('nutrition'):\n",
        "        print(f\"Nutrition: {recipe['nutrition']['calories']:.0f} cal, {recipe['nutrition']['protein_g']:.1f}g protein\")\n",
        "    print(f\"Regions: {', '.join(recipe.get('regions', []))}\")\n",
        "    print(f\"Allergens: {', '.join(recipe.get('allergens', [])) if recipe.get('allergens') else 'None detected'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING FINAL DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save final validated dataset\n",
        "output_file = 'final_recipes_enriched.jsonl'\n",
        "print(f\"\\nSaving {len(valid_recipes):,} recipes to {output_file}...\")\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for recipe in tqdm(valid_recipes, desc=\"Saving\"):\n",
        "        f.write(json.dumps(recipe) + '\\n')\n",
        "\n",
        "print(f\"✓ Saved to: {output_file}\")\n",
        "\n",
        "# Save sample for testing\n",
        "sample_recipes = valid_recipes[:1000]\n",
        "with open('sample_recipes_1k.jsonl', 'w') as f:\n",
        "    for recipe in sample_recipes:\n",
        "        f.write(json.dumps(recipe) + '\\n')\n",
        "\n",
        "print(f\"✓ Saved 1,000 sample recipes to: sample_recipes_1k.jsonl\")\n",
        "\n",
        "# Save statistics\n",
        "import datetime\n",
        "\n",
        "final_stats = {\n",
        "    'total_recipes': len(valid_recipes),\n",
        "    'filtered_out': len(filtered_out),\n",
        "    'sources': {\n",
        "        'food.com': sum(1 for r in valid_recipes if r.get('source') == 'food.com'),\n",
        "        'epicurious': sum(1 for r in valid_recipes if r.get('source') == 'epicurious')\n",
        "    },\n",
        "    'cuisines': sorted(list(set([r['cuisine'] for r in valid_recipes]))),\n",
        "    'num_cuisines': len(set([r['cuisine'] for r in valid_recipes])),\n",
        "    'regions': sorted(list(set([r for recipe in valid_recipes for r in recipe.get('regions', [])]))),\n",
        "    'allergens': sorted(list(set([a for recipe in valid_recipes for a in recipe.get('allergens', [])]))),\n",
        "    'export_date': str(datetime.datetime.now()),\n",
        "    'files': {\n",
        "        'main': output_file,\n",
        "        'sample': 'sample_recipes_1k.jsonl'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('dataset_final_stats.json', 'w') as f:\n",
        "    json.dump(final_stats, f, indent=2)\n",
        "\n",
        "print(\"✓ Saved statistics to: dataset_final_stats.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ VALIDATION & EXPORT COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nFiles created:\")\n",
        "print(f\"  1. {output_file} - Final dataset ({len(valid_recipes):,} recipes)\")\n",
        "print(f\"  2. sample_recipes_1k.jsonl - Test sample (1,000 recipes)\")\n",
        "print(f\"  3. dataset_final_stats.json - Statistics\")\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"  ✓ {len(valid_recipes):,} valid recipes\")\n",
        "print(f\"  ✓ {stats['cuisines']} unique cuisines\")\n",
        "print(f\"  ✓ {len(region_dist)} regions\")\n",
        "print(f\"  ✓ 100% have nutrition data\")\n",
        "print(f\"  ✓ Ready for ChromaDB ingestion!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ READY FOR NEXT STEP: CHROMADB INGESTION!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvVSKpH676f5",
        "outputId": "2fb7d97e-09a6-4144-f9b8-160bf3b2ed00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MERGING DATASETS & QUALITY CHECKS (FIXED)\n",
            "============================================================\n",
            "\n",
            "[1/7] Loading Food.com recipes...\n",
            "✓ Loaded 226,101 Food.com recipes\n",
            "\n",
            "[2/7] Loading Epicurious recipes...\n",
            "✓ Loaded 20,052 Epicurious recipes\n",
            "\n",
            "[3/7] Converting Food.com to standard format and adding features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Food.com: 100%|██████████| 226101/226101 [00:10<00:00, 20601.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Processed 226,101 Food.com recipes\n",
            "\n",
            "[4/7] Adding features to Epicurious recipes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epicurious: 100%|██████████| 20052/20052 [00:00<00:00, 235043.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Processed 20,052 Epicurious recipes\n",
            "\n",
            "[5/7] Merging datasets...\n",
            "✓ Total recipes before filtering: 246,153\n",
            "  - Food.com: 226,101\n",
            "  - Epicurious: 20,052\n",
            "\n",
            "[6/7] Filtering invalid recipes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Filtering: 100%|██████████| 246153/246153 [00:00<00:00, 515711.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Valid recipes: 226,100\n",
            "✗ Filtered out: 20,053\n",
            "\n",
            "Reasons for filtering:\n",
            "  no_title: 1\n",
            "  no_ingredients: 20,052\n",
            "\n",
            "============================================================\n",
            "FINAL DATASET STATISTICS\n",
            "============================================================\n",
            "\n",
            "Dataset Size:\n",
            "  Total recipes: 226,100\n",
            "  Food.com: 226,100\n",
            "  Epicurious: 0\n",
            "\n",
            "Content Stats:\n",
            "  Unique cuisines: 20\n",
            "  Avg ingredients: 9.2\n",
            "  Avg time: 9618 minutes\n",
            "\n",
            "Data Completeness:\n",
            "  Has nutrition: 226,100 (100.0%)\n",
            "  Has cuisine: 226,100 (100%)\n",
            "  Has regions: 226,100 (100.0%)\n",
            "  Has allergens: 182,855 (80.9%)\n",
            "  Has difficulty: 0 (0.0%)\n",
            "\n",
            "============================================================\n",
            "CUISINE DISTRIBUTION\n",
            "============================================================\n",
            "\n",
            "Top 15 cuisines:\n",
            "  southern_us    : 99,653 ( 44.1%)\n",
            "  italian        : 64,696 ( 28.6%)\n",
            "  mexican        : 30,026 ( 13.3%)\n",
            "  chinese        :  9,323 (  4.1%)\n",
            "  indian         :  7,906 (  3.5%)\n",
            "  french         :  4,582 (  2.0%)\n",
            "  greek          :  2,205 (  1.0%)\n",
            "  thai           :  2,071 (  0.9%)\n",
            "  cajun_creole   :  1,906 (  0.8%)\n",
            "  japanese       :  1,252 (  0.6%)\n",
            "  moroccan       :  1,138 (  0.5%)\n",
            "  korean         :    536 (  0.2%)\n",
            "  filipino       :    215 (  0.1%)\n",
            "  vietnamese     :    212 (  0.1%)\n",
            "  jamaican       :    129 (  0.1%)\n",
            "\n",
            "Difficulty distribution:\n",
            "  unknown: 226,100 (100.0%)\n",
            "\n",
            "Region distribution:\n",
            "  global: 221,206\n",
            "  north_america: 101,559\n",
            "  europe: 69,497\n",
            "  latin_america: 30,186\n",
            "  asia: 21,515\n",
            "  mediterranean: 2,205\n",
            "  middle_east: 1,138\n",
            "\n",
            "Allergen distribution:\n",
            "  dairy: 141,905\n",
            "  wheat: 79,128\n",
            "  eggs: 70,288\n",
            "  tree_nuts: 24,413\n",
            "  soy: 14,241\n",
            "  shellfish: 8,715\n",
            "  fish: 8,705\n",
            "  peanuts: 7,366\n",
            "\n",
            "============================================================\n",
            "NUTRITION STATISTICS\n",
            "============================================================\n",
            "\n",
            "Calories:\n",
            "  Mean: 475\n",
            "  Median: 316\n",
            "  Range: 0 - 434360\n",
            "\n",
            "Protein (g):\n",
            "  Mean: 17.5\n",
            "  Median: 9.5\n",
            "\n",
            "Carbs (g):\n",
            "  Mean: 42.8\n",
            "  Median: 24.8\n",
            "\n",
            "Fats (g):\n",
            "  Mean: 28.2\n",
            "  Median: 16.4\n",
            "\n",
            "============================================================\n",
            "SAMPLE VALID RECIPES\n",
            "============================================================\n",
            "\n",
            "Title: chunky peanut butter overload cookies\n",
            "Source: food.com\n",
            "Cuisine: southern_us (confidence: 41.90%)\n",
            "Ingredients (9): all-purpose flour, baking soda, salt, vegetable shortening, creamy peanut butter...\n",
            "Nutrition: 192 cal, 2.5g protein\n",
            "Regions: global, north_america\n",
            "Allergens: dairy, eggs, peanuts, tree_nuts, wheat\n",
            "\n",
            "Title: spicy lamb tagine with couscous\n",
            "Source: food.com\n",
            "Cuisine: moroccan (confidence: 35.90%)\n",
            "Ingredients (18): lean lamb leg steak, onions, garlic, cayenne pepper, cinnamon sticks...\n",
            "Nutrition: 264 cal, 8.0g protein\n",
            "Regions: middle_east\n",
            "Allergens: wheat\n",
            "\n",
            "Title: broken glass salad\n",
            "Source: food.com\n",
            "Cuisine: mexican (confidence: 21.00%)\n",
            "Ingredients (9): lime jell-o gelatin, boiling water, cold water, strawberry jell-o gelatin dessert, orange jell-o...\n",
            "Nutrition: 154 cal, 1.5g protein\n",
            "Regions: global, latin_america\n",
            "Allergens: None detected\n",
            "\n",
            "============================================================\n",
            "SAVING FINAL DATASET\n",
            "============================================================\n",
            "\n",
            "Saving 226,100 recipes to final_recipes_enriched.jsonl...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving: 100%|██████████| 226100/226100 [00:11<00:00, 19493.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved to: final_recipes_enriched.jsonl\n",
            "✓ Saved 1,000 sample recipes to: sample_recipes_1k.jsonl\n",
            "✓ Saved statistics to: dataset_final_stats.json\n",
            "\n",
            "============================================================\n",
            "✅ VALIDATION & EXPORT COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Files created:\n",
            "  1. final_recipes_enriched.jsonl - Final dataset (226,100 recipes)\n",
            "  2. sample_recipes_1k.jsonl - Test sample (1,000 recipes)\n",
            "  3. dataset_final_stats.json - Statistics\n",
            "\n",
            "Dataset Summary:\n",
            "  ✓ 226,100 valid recipes\n",
            "  ✓ 20 unique cuisines\n",
            "  ✓ 7 regions\n",
            "  ✓ 100% have nutrition data\n",
            "  ✓ Ready for ChromaDB ingestion!\n",
            "\n",
            "============================================================\n",
            "✅ READY FOR NEXT STEP: CHROMADB INGESTION!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING FILES TO GOOGLE DRIVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Mount Google Drive (should already be mounted)\n",
        "print(\"\\n[1/4] Mounting Google Drive...\")\n",
        "# Removed drive.mount() call as it's already mounted and called in the first cell\n",
        "print(\"✓ Google Drive is already mounted!\")\n",
        "\n",
        "# Set the correct working directory where files were generated\n",
        "# This assumes the files were generated in the directory set by the initial os.chdir command\n",
        "current_working_dir = '/content/drive/MyDrive/Recipe_dataset'\n",
        "print(f\"\\nUsing detected file generation directory: {current_working_dir}\")\n",
        "\n",
        "# Create backup folder with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup_folder = f'/content/drive/MyDrive/culinary_assistant_backup_{timestamp}'\n",
        "\n",
        "print(f\"\\n[2/4] Creating backup folder...\")\n",
        "os.makedirs(backup_folder, exist_ok=True)\n",
        "print(f\"✓ Created: {backup_folder}\")\n",
        "\n",
        "# List of files to backup\n",
        "files_to_backup = {\n",
        "    # Final datasets\n",
        "    'final_recipes_enriched.jsonl': 'Main dataset with all recipes',\n",
        "    'sample_recipes_1k.jsonl': 'Sample dataset (1,000 recipes)',\n",
        "    'dataset_final_stats.json': 'Dataset statistics',\n",
        "\n",
        "    # Intermediate files\n",
        "    'food_com_with_cuisines.jsonl': 'Food.com with ML-predicted cuisines',\n",
        "    'epicurious_processed.jsonl': 'Processed Epicurious recipes',\n",
        "\n",
        "    # Model files\n",
        "    'cuisine_classifier.pkl': 'Trained cuisine classification model',\n",
        "\n",
        "    # Source data (optional)\n",
        "    # 'food_com_cleaned.jsonl': 'Original cleaned Food.com data',\n",
        "    # 'train.json': 'Yummly dataset',\n",
        "    # 'epi_r.csv': 'Epicurious raw data',\n",
        "}\n",
        "\n",
        "print(f\"\\n[3/4] Copying files to Google Drive...\")\n",
        "print(f\"Backup location: {backup_folder}\\n\")\n",
        "\n",
        "copied_files = []\n",
        "missing_files = []\n",
        "total_size = 0\n",
        "\n",
        "for filename, description in files_to_backup.items():\n",
        "    # Construct source path using the current working directory\n",
        "    source_path = os.path.join(current_working_dir, filename)\n",
        "    dest_path = os.path.join(backup_folder, filename)\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        # Get file size\n",
        "        size_mb = os.path.getsize(source_path) / (1024 * 1024)\n",
        "        total_size += size_mb\n",
        "\n",
        "        # Copy file\n",
        "        print(f\"Copying: {filename} ({size_mb:.1f} MB)\")\n",
        "        print(f\"  → {description}\")\n",
        "        shutil.copy2(source_path, dest_path)\n",
        "        copied_files.append(filename)\n",
        "        print(f\"  ✓ Copied successfully!\")\n",
        "    else:\n",
        "        print(f\"⚠ Skipping: {filename} (not found at {source_path})\")\n",
        "        missing_files.append(filename)\n",
        "    print()\n",
        "\n",
        "print(\"\\n[4/4] Creating backup manifest...\")\n",
        "\n",
        "# Create a manifest file with details\n",
        "manifest_content = f\"\"\"Culinary Assistant - Data Backup\\nGenerated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\nBackup Location: {backup_folder}\\n\\nFILES BACKED UP ({len(copied_files)}):\\n{'='*60}\\n\"\"\"\n",
        "\n",
        "for filename in copied_files:\n",
        "    filepath = os.path.join(backup_folder, filename)\n",
        "    size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "    manifest_content += f\"\\n{filename}\\n\"\n",
        "    manifest_content += f\"  Size: {size_mb:.2f} MB\\n\"\n",
        "    manifest_content += f\"  Description: {files_to_backup[filename]}\\n\"\n",
        "\n",
        "if missing_files:\n",
        "    manifest_content += f\"\\n\\nFILES NOT FOUND ({len(missing_files)}):\\n\"\n",
        "    manifest_content += \"=\"*60 + \"\\n\"\n",
        "    for filename in missing_files:\n",
        "        manifest_content += f\"\\n{filename}\\n\"\n",
        "        manifest_content += f\"  Description: {files_to_backup[filename]}\\n\"\n",
        "\n",
        "manifest_content += f\"\\n\\nTOTAL SIZE: {total_size:.2f} MB\\n\"\n",
        "\n",
        "# Save manifest\n",
        "manifest_path = os.path.join(backup_folder, 'BACKUP_MANIFEST.txt')\n",
        "with open(manifest_path, 'w') as f:\n",
        "    f.write(manifest_content)\n",
        "\n",
        "print(\"✓ Manifest created!\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BACKUP COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n📁 Backup Location:\")\n",
        "print(f\"   {backup_folder}\")\n",
        "\n",
        "print(f\"\\n✓ Files backed up: {len(copied_files)}\")\n",
        "print(f\"✓ Total size: {total_size:.2f} MB\")\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\n⚠ Files not found: {len(missing_files)}\")\n",
        "    for filename in missing_files:\n",
        "        print(f\"   - {filename}\")\n",
        "\n",
        "print(f\"\\n📄 See BACKUP_MANIFEST.txt for complete details\")\n",
        "\n",
        "# Create a shortcut path for easy access\n",
        "easy_path = '/content/drive/MyDrive/culinary_assistant_LATEST'\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING EASY ACCESS FOLDER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Copy to \"LATEST\" folder (overwrites previous)\n",
        "if os.path.exists(easy_path):\n",
        "    shutil.rmtree(easy_path)\n",
        "\n",
        "shutil.copytree(backup_folder, easy_path)\n",
        "print(f\"\\n✓ Also saved to: {easy_path}\")\n",
        "print(\"  (This is always your latest backup)\")\n",
        "\n",
        "# Display file tree\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GOOGLE DRIVE FOLDER STRUCTURE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nMyDrive/\")\n",
        "print(f\"├── culinary_assistant_backup_{timestamp}/\")\n",
        "for filename in copied_files:\n",
        "    print(f\"│   ├── {filename}\")\n",
        "print(f\"│   └── BACKUP_MANIFEST.txt\")\n",
        "print(f\"│\")\n",
        "print(f\"└── culinary_assistant_LATEST/  ← Easy access\")\n",
        "for filename in copied_files:\n",
        "    print(f\"    ├── {filename}\")\n",
        "print(f\"    └── BACKUP_MANIFEST.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ ALL FILES SAVED TO GOOGLE DRIVE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n💡 Access your files:\")\n",
        "print(\"   1. Open Google Drive in browser\")\n",
        "print(\"   2. Navigate to 'culinary_assistant_LATEST' folder\")\n",
        "print(\"   3. Download any file you need\")\n",
        "\n",
        "print(\"\\n💡 To restore in a new Colab session:\")\n",
        "print(\"   from google.colab import drive\")\n",
        "print(\"   drive.mount('/content/drive')\")\n",
        "print(\"   !cp /content/drive/MyDrive/culinary_assistant_LATEST/*.jsonl /content/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwI3xWszzbDu",
        "outputId": "86a2749e-63b6-41d2-984a-66b34177fcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SAVING FILES TO GOOGLE DRIVE\n",
            "============================================================\n",
            "\n",
            "[1/4] Mounting Google Drive...\n",
            "✓ Google Drive is already mounted!\n",
            "\n",
            "Using detected file generation directory: /content/drive/MyDrive/Recipe_dataset\n",
            "\n",
            "[2/4] Creating backup folder...\n",
            "✓ Created: /content/drive/MyDrive/culinary_assistant_backup_20260210_144455\n",
            "\n",
            "[3/4] Copying files to Google Drive...\n",
            "Backup location: /content/drive/MyDrive/culinary_assistant_backup_20260210_144455\n",
            "\n",
            "Copying: final_recipes_enriched.jsonl (387.8 MB)\n",
            "  → Main dataset with all recipes\n",
            "  ✓ Copied successfully!\n",
            "\n",
            "Copying: sample_recipes_1k.jsonl (1.7 MB)\n",
            "  → Sample dataset (1,000 recipes)\n",
            "  ✓ Copied successfully!\n",
            "\n",
            "Copying: dataset_final_stats.json (0.0 MB)\n",
            "  → Dataset statistics\n",
            "  ✓ Copied successfully!\n",
            "\n",
            "Copying: food_com_with_cuisines.jsonl (318.7 MB)\n",
            "  → Food.com with ML-predicted cuisines\n",
            "  ✓ Copied successfully!\n",
            "\n",
            "Copying: epicurious_processed.jsonl (11.9 MB)\n",
            "  → Processed Epicurious recipes\n",
            "  ✓ Copied successfully!\n",
            "\n",
            "Copying: cuisine_classifier.pkl (202.9 MB)\n",
            "  → Trained cuisine classification model\n",
            "  ✓ Copied successfully!\n",
            "\n",
            "\n",
            "[4/4] Creating backup manifest...\n",
            "✓ Manifest created!\n",
            "\n",
            "============================================================\n",
            "BACKUP COMPLETE!\n",
            "============================================================\n",
            "\n",
            "📁 Backup Location:\n",
            "   /content/drive/MyDrive/culinary_assistant_backup_20260210_144455\n",
            "\n",
            "✓ Files backed up: 6\n",
            "✓ Total size: 923.10 MB\n",
            "\n",
            "📄 See BACKUP_MANIFEST.txt for complete details\n",
            "\n",
            "============================================================\n",
            "CREATING EASY ACCESS FOLDER\n",
            "============================================================\n",
            "\n",
            "✓ Also saved to: /content/drive/MyDrive/culinary_assistant_LATEST\n",
            "  (This is always your latest backup)\n",
            "\n",
            "============================================================\n",
            "GOOGLE DRIVE FOLDER STRUCTURE\n",
            "============================================================\n",
            "\n",
            "MyDrive/\n",
            "├── culinary_assistant_backup_20260210_144455/\n",
            "│   ├── final_recipes_enriched.jsonl\n",
            "│   ├── sample_recipes_1k.jsonl\n",
            "│   ├── dataset_final_stats.json\n",
            "│   ├── food_com_with_cuisines.jsonl\n",
            "│   ├── epicurious_processed.jsonl\n",
            "│   ├── cuisine_classifier.pkl\n",
            "│   └── BACKUP_MANIFEST.txt\n",
            "│\n",
            "└── culinary_assistant_LATEST/  ← Easy access\n",
            "    ├── final_recipes_enriched.jsonl\n",
            "    ├── sample_recipes_1k.jsonl\n",
            "    ├── dataset_final_stats.json\n",
            "    ├── food_com_with_cuisines.jsonl\n",
            "    ├── epicurious_processed.jsonl\n",
            "    ├── cuisine_classifier.pkl\n",
            "    └── BACKUP_MANIFEST.txt\n",
            "\n",
            "============================================================\n",
            "✅ ALL FILES SAVED TO GOOGLE DRIVE!\n",
            "============================================================\n",
            "\n",
            "💡 Access your files:\n",
            "   1. Open Google Drive in browser\n",
            "   2. Navigate to 'culinary_assistant_LATEST' folder\n",
            "   3. Download any file you need\n",
            "\n",
            "💡 To restore in a new Colab session:\n",
            "   from google.colab import drive\n",
            "   drive.mount('/content/drive')\n",
            "   !cp /content/drive/MyDrive/culinary_assistant_LATEST/*.jsonl /content/\n"
          ]
        }
      ]
    }
  ]
}